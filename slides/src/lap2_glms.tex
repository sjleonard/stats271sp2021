\documentclass[aspectratio=169]{beamer}
\usetheme{simple}

\input{preamble/preamble.tex}
\input{preamble/preamble_math.tex}

\title{STATS271/371: Applied Bayesian Statistics}
\subtitle{Lap 2: Generalized Linear Models and the Laplace Approximation}
\author{Scott Linderman}
\date{\today}

\begin{document}

\maketitle


\begin{frame}{Announcements}

\begin{itemize}
    \item Lecture slides and course content on Github: \url{https://github.com/slinderman/stats271sp2021}
    
    \item Homework 2 due Friday (April 16).
\end{itemize}
    
\end{frame}

\begin{frame}{Final Projects}
    \begin{itemize}
        \item \textbf{Goal:} Go through Box's Loop on a real dataset.
        \begin{itemize}
        \item Find an interesting real dataset, e.g. from your own research, a recent paper, or a public repository. (Synthetic datasets are not sufficient.)
        \item Develop a model. 
        \item Think carefully about Bayesian inference algorithms and run one.
        \item Evaluate your model, then revise it in light of your analysis. 
        \item Repeat...
        \item Present your results and findings in the form of a short report. 
        \end{itemize}
        
    \item You may work individually or in a team of two.
    \item Proposals due late April/early May (TBD).
    \end{itemize}
\end{frame}

\begin{frame}{Survey Results}
\textit{Where are you zooming in from?}
\begin{itemize}
    \item Campus or nearby: 24/29 (!!!)
    \item San Francisco
    \item India
    \item Senatobia, MS
    \item North Carolina
    \item Houston, TX
\end{itemize}
\end{frame}

\begin{frame}{Survey Results II}
\centering
\includegraphics[width=.9\textwidth]{figures/lap2/survey-pl.pdf}
\end{frame}

\begin{frame}{Survey Results III}
\centering
\includegraphics[width=.75\textwidth]{figures/lap2/survey.pdf}
\end{frame}

\begin{frame}{Survey Results IV}
\textit{Any pandemic hobbies?}
\begin{columns}
\begin{column}{.5\textwidth}
\begin{itemize}
    \item Fingerstyle guitar
    \item Cycling
    \item Hiking
    \item Playing music
    \item Rock hard abs
    \item Golf
    \item Jouranling / Tik Tok
    \item Tennis (x3)
    \item Checking Covid numbers :(
    \item Cooking (x3)
\end{itemize}
\end{column}
\begin{column}{.5\textwidth}
\begin{itemize}
    \item Reading (x2)
    \item Looking after my newborn :)
    \item Astrophotography
    \item Yoga/Pokemon Go
    \item Chess
    \item Learning Spanish
    \item Dominion
    \item Walking
    \item Piano
\end{itemize}
\end{column}

\end{columns}
\end{frame}


\begin{frame}{Lap 2 of Box's Loop}
\begin{center}
\includegraphics[width=.85\linewidth]{figures/lap1/boxsloop.jpeg}\\
\end{center} 
\begin{flushright}
{\footnotesize Blei, \textit{Ann. Rev. Stat. App.} 2014.}
\end{flushright}
\end{frame}


\begin{frame}{Lap 2 of Box's Loop}
\centering 
\includegraphics[width=.8\linewidth]{figures/lap2/ferrari.png}
{\footnotesize \url{https://www.formula1.com/en/racing/2021/EmiliaRomagna/Circuit.html}}
\end{frame}

\begin{frame}{Bayesian Generalized Linear Models}

Lap 2 of Box's loop will introduce:
\begin{itemize}
    \item \textbf{Model:} Bayesian generalized linear models
    \item \textbf{Algorithm:} Laplace approximation
    \item \textbf{Criticism:} Posterior predictive checks
\end{itemize}

\end{frame}

\begin{frame}{Motivation}

Linear regression assumed real-valued observations, but what if our data has some other support?
\begin{itemize}
    \item $y_n \in \{0, 1\}$ binary observation; e.g. win/lose, healthy/sick.
    \item $y_n \in \{0, 1, 2, \ldots\} $ count observation; e.g. number of spikes in 25ms bin.
    \item $y_n \in \{0, 1, \ldots, M\} $ count observation (with max); e.g. number of races won out of $M$ total
    \item $y_n \in \{1, \ldots, K\}$ unordered, categorical observation; e.g. 
    \begin{itemize}
        \item Democrat/Republican/Independent
        \item Hamilton/Madison/Jay (authors of the Federalist papers)
        \item Hamilton/Verstappen/Bottas (only F1 drivers who win races)
        \item Stanford/Berkeley/Not-Stanford-or-Berkeley (only institutions of higher learning)
    \end{itemize}
\end{itemize}

We could still apply linear regression, but it would make weird predictions. \textit{There will be -2 spikes in the next time bin?} \textit{Bet on Hamilstappen to win?}
\end{frame}



\begin{frame}[t]{Notation}

Let 
\begin{itemize}
    \item $y_n \in \cY$ denote the $n$-th \textit{observation} (type to be specified later)
    \item $\mbx_n \in \reals^P$ denote a the \textit{covariates} (aka features) correspond the $n$-th datapoint
    \item $\mbw \in \reals^P$ denote the \textit{weights} of the model
\end{itemize}

\end{frame}

\begin{frame}{Example: Bernoulli GLM}

\begin{columns}

\begin{column}{.5\textwidth}
\begin{itemize}
\item For example, consider modeling binary observations $y_n \in \{0,1\}$ given covariates $\mbx_n \in \reals^P$.

\item We model the observations as Bernoulli random variables, 
\begin{align}
    y_n \mid \mbx_n &\sim \distBernoulli(p_n)
\end{align}
where
\begin{align}
    p_n &= \Pr(y_n = 1 \mid \mbx_n)
    = \E[y_n \mid \mbx_n].
\end{align}

\item Then, model the conditional expectation as $\E[y_n \mid \mbx_n] = f(\mbw^\top \mbx_n)$ where $f: \reals \to [0,1]$ is the \emph{mean function}.

\end{itemize}

\end{column}

\begin{column}{.5\textwidth}
\includegraphics[width=1.\linewidth]{figures/lap2/data.pdf}
\end{column}

\end{columns}
\end{frame}

\begin{frame}{Logistic and probit functions}
Common choices for $f$ include the logistic and probit functions, corresponding to the \emph{logistic regression} and \emph{probit regression} models, respectively.

\centering
\includegraphics[width=.7\textwidth]{figures/lap2/logistic_probit.pdf}
\end{frame}

\begin{frame}{Likelihood}
The likelihood of $N$ conditionally independent observations is,
\begin{align}
    p(\{y_n\}_{n=1}^N \mid \{\mbx_n\}_{n=1}^N, \mbw) 
    &= 
    \prod_{n=1}^N \distBernoulli(y_n \mid f(\mbw^\top \mbx_n)) \\
    &= \prod_{n=1}^N f(\mbw^\top \mbx_n)^{y_n} \, (1-f(\mbw^\top \mbx_n))^{1-y_n}
\end{align}
For example, consider the \textit{logistic function} $f(a) = \frac{e^a}{1+e^{a}}$. Then,
\begin{align}
    p(\{y_n\}_{n=1}^N \mid \{\mbx_n\}_{n=1}^N, \mbw) 
    &= \prod_{n=1}^N \left[ \frac{e^{\mbw^\top \mbx_n}}{1+e^{\mbw^\top \mbx_n}} \right]^{y_n} 
    \left[ \frac{1}{1+e^{\mbw^\top \mbx_n}} \right]^{1-y_n}
    \\
    \label{eq:ch2_lkhd}
    &= \prod_{n=1}^N \frac{e^{\langle y_n \mbx_n, \, \mbw \rangle}}{1+e^{\mbw^\top \mbx_n}}
\end{align}

The numerator can be written in terms of $\sum_n y_n \mbx_n$, but the denominator doesn't easily simplify. 

\end{frame}


\begin{frame}{Prior}

\begin{itemize}

\item Here, we'll use a spherical multivariate normal prior,
\begin{align}
    p(\mbw) &= \cN(\mbw \mid \mbzero, \sigma^2 \mbI),
\end{align}
where $\mbzero \in \reals^P$ and $\sigma^2 \in \reals_+$ are the mean and variance, respectively.

\item This is the simplest choice, but we have lots of flexibility if a stronger prior is warranted.

\item For example, it's easy to generalize to arbitrary mean and covariance, $\mbmu$ and $\mbSigma$.

\end{itemize}

\end{frame}

\begin{frame}{Log probability contours}
    \centering
    \includegraphics[width=.75\textwidth]{figures/lap2/log_joint.pdf}
\end{frame}

\begin{frame}{Box's Loop: Infer hidden quantities}
\begin{center}
\includegraphics[width=.85\linewidth]{figures/lap1/boxsloop.jpeg}\\
\end{center} 
\begin{flushright}
{\footnotesize Blei, \textit{Ann. Rev. Stat. App.} 2014.}
\end{flushright}
\end{frame}


\begin{frame}{Maximum a posteriori (MAP) estimation}

Before going to full Bayesian inference, let's just look for the posterior mode.

The log posterior probability is equal to the log joint minus the log marginal likelihood, which is constant with respect to the parameters $\mbw$,
\begin{align}
    \cL(\mbw) &= \log p(\{y_n\}_{n=1}^N \mid \{\mbx_n\}_{n=1}^N, \mbw) + \log p(\mbw) - \underbrace{\log p(\{y_n\}_{n=1}^N \mid \{\mbx_n\}_{n=1}^N)}_{\text{const. w.r.t. } \mbw} \\
    &= \sum_{n=1}^N \log \frac{e^{\langle y_n \mbx_n, \, \mbw \rangle}}{1+e^{\mbw^\top \mbx_n}} -\frac{1}{2\sigma^2} \mbw^\top \mbw + c\\
    &= \Big \langle \sum_{n=1}^N y_n \mbx_n, \mbw \Big \rangle 
    - \sum_{n=1}^N \log (1 + \exp \{\mbx_n^\top \mbw \}) - \frac{1}{2\sigma^2} \mbw^\top \mbw + c
\end{align}

\end{frame}

\begin{frame}{Gradient of the log probability}

Taking the gradient and setting it to zero,
\begin{align}
    \nabla_{\mbw} \cL(\mbw) &= 
    \sum_{n=1}^N y_n \mbx_n - \sum_{n=1}^N 
    \frac{\exp\{\mbx_n^\top \mbw\}}{1 + \exp \{\mbx_n^\top \mbw\}} \mbx_n
    - \frac{1}{\sigma^2} \mbw
    \\
    &= \sum_{n=1}^N y_n \mbx_n - \sum_{n=1}^N 
    f(\mbx_n^\top \mbw) \mbx_n.
    - \frac{1}{\sigma^2} \mbw
\end{align}
In matrix notation, 
\begin{align}
    \mbX = \begin{bmatrix} - & \mbx_1^\top & - \\ & \vdots & \\ - & \mbx_N^\top & - \end{bmatrix}, \quad \text{and} \quad  &
    \mby = \begin{bmatrix} y_1 \\ \vdots \\ y_N \end{bmatrix},
\end{align}
and letting $\hat{\mby} = f(\mbX \mbw)$, where $f$ is applied elementwise, we can write the gradient as,
\begin{align}
    \nabla_{\mbw} \cL(\mbw) &= (\mby - \hat{\mby})^\top \mbX - \frac{1}{\sigma^2} \mbw.
\end{align}
\end{frame}

\begin{frame}{Gradient of the log probability II}
\begin{itemize}
    \item In other words, the \textit{gradient of the log likelihood is a weighted sum of the covariates} where the weights are given by the error $y_n - \hat{y}_n$.
    
    \item Note that the gradient is still a \textit{nonlinear function of $\mbw$} since $\hat{y} = f(\mbx_n^\top \mbw)$. 
    
    \item The nonlinear equation $\nabla_{\mbw} \cL(\mbw) = 0$ does not have a closed form solution.
\end{itemize}


\end{frame}

\begin{frame}{Hessian of the log probability}

\begin{itemize}
\item The Hessian---i.e. the matrix of second partial derivatives, or the Jacobian of the gradient---is,
\begin{align}
    \nabla_{\mbw}^2 \cL(\mbw) &= 
    \nabla_{\mbw} \sum_{n=1}^N \Big(y_n - f(\mbx_n^\top \mbw) \Big) \mbx_n 
    - \frac{1}{\sigma^2} \mbw.
    \\
    &= -\left( \sum_{n=1}^N f'(\mbx_n^\top \mbw) \mbx_n \mbx_n^\top + \frac{1}{\sigma^2} \mbI \right)
\end{align}
where $f': \reals \to \reals$ is the derivative of $f$. 

\item Since $f$ is monotonically increasing, $f' \geq 0$.

\item The negative Hessian is a weighted sum of outer products with positive weights $f'(\mbx_n^\top \mbw)$. Thus, the Hessian is negative definite.

\item That means the \textit{log probability is concave} and has a global optimum.

\end{itemize}
\end{frame}

\begin{frame}{Concavity of the log probability}
    \centering
    \includegraphics[width=.75\textwidth]{figures/lap2/log_joint_1d.pdf}
\end{frame}


\begin{frame}[t]{Exercise: The derivative of the logistic function}

\textit{Show that}
\begin{align}
    f'(a) = \left[\frac{\dif}{\dif \alpha} \frac{e^\alpha}{1+e^\alpha}\right]_{\alpha=a} = f(a) (1 - f(a)) = f(a)f(-a) \geq 0.
\end{align}

\end{frame}


\begin{frame}{Algorithm}
\begin{itemize}

\item Though the MAP estimate doesn't have a closed form solution, we can find it via optimization. 

\item Since the log probability is concave, standard optimization methods will find the global optimum. 

\item For example, (damped) \emph{Newton's method},
\begin{align}
    \mbw^{(i+1)} &\leftarrow \mbw^{(i)} - \gamma [\nabla^2_{\mbw} \cL(\mbw^{(i)})]^{-1} \nabla_{\mbw} \cL(\mbw^{(i)}) 
\end{align}
(with sufficiently small step size $\gamma \in (0, 1]$) achieves second-order convergence to the optimum,~$\mbw_{\mathsf{MAP}}$.

\item With many covariates (when $P$ is large), Newton's method incurs an $O(P^3)$ complexity. 

\item \emph{Quasi}-Newton methods, like the BFGS algorithm, build an approximation to approximate Newton's method at lower computational cost. 

\end{itemize}

\end{frame}

\begin{frame}{BFGS Convergence}
    \centering
    \includegraphics[width=.75\textwidth]{figures/lap2/bfgs.pdf}
\end{frame}


\begin{frame}{Posterior Distribution}
What if we want more than a point estimate? 

The posterior distribution is given by,
\begin{align}
    p(\mbw \mid \{\mbx_n, y_n\}_{n=1}^N) &= \frac{p(\mbw) \, p(\{y_n\}_{n=1}^N \mid \mbw, \{\mbx_n\}_{n=1}^N)}{p(\{y_n\}_{n=1}^N \mid \{\mbx_n\}_{n=1}^N)} \\
\end{align}

Unfortunately, the marginal likelihood is hard to compute since 
\begin{align}
    p(\{y_n\}_{n=1}^N \mid \{\mbx_n\}_{n=1}^N) &=
    \int \prod_{n=1}^N \mathrm{Bern}(y_n \mid f(\mbw^\top \mbx_n)) \, \mathcal{N}(\mbw \mid 0, \sigma^2 \mbI) \dif \mbw
\end{align}
does not have a closed form solution.

\end{frame}

\begin{frame}{Laplace Approximation}

\textbf{Idea: } \textit{approximate the posterior with a multivariate normal distribution centered on the mode.}

To motivate this, consider a second-order Taylor approximation to the log posterior,
\begin{align}
    \cL(\mbw) &\approx \cL(\mbw_{\mathsf{MAP}}) + (\mbw - \mbw_{\mathsf{MAP}})^\top \underbrace{\nabla_{\mbw} \cL(\mbw_{\mathsf{MAP}})}_{\mbzero \text{ at the mode}} + \frac{1}{2} (\mbw - \mbw_{\mathsf{MAP}})^\top \nabla^2_{\mbw} \cL(\mbw_{\mathsf{MAP}}) (\mbw - \mbw_{\mathsf{MAP}}) \\
    &= -\frac{1}{2} (\mbw - \mbw_{\mathsf{MAP}})^\top \mbSigma^{-1} (\mbw - \mbw_{\mathsf{MAP}}) + c \\
    &= \log \cN(\mbw \mid \mbw_{\mathsf{MAP}}, \mbSigma) 
\end{align}
where $\mbSigma = - [\nabla^2_{\mbw} \cL(\mbw_{\mathsf{MAP}})]^{-1} $

In other words, the posterior is approximately Gaussian with covariance given by the (negative) inverse Hessian at the mode.

Since the Hessian is \textit{negative} definite, the covariance is \emph{positive} definite, as required.
\end{frame}

\begin{frame}{Laplace Approximation II}
    
    \centering
    \includegraphics[width=.75\textwidth]{figures/lap2/laplace.pdf}
\end{frame}


\begin{frame}{Bernstein-von Mises Theorem}
    In the large data limit (as $N \to \infty$), the posterior is asymptotically normal, justifying the Laplace approximation in this regime.
    
    Consider a simpler setting in which we have data $\{y_n\}_{n=1}^N \iid{\sim} p(y \mid \theta_{\mathsf{true}})$.
    
    Under some conditions (e.g. $\theta_{\mathsf{true}}$ not on the boundary of $\Theta$ and $\theta_{\mathsf{true}}$ has nonzero prior probability), then the MAP estimate is consistent. As $N \to \infty$,  $\theta_{\mathsf{MAP}} \to \theta_{\mathsf{true}}$.
    
    Likewise,
    \begin{align} 
        p(\theta \mid \{y_n\}_{n=1}^N) \to \cN \big(\theta \mid \theta_{\mathsf{true}}, \tfrac{1}{N} [J(\theta_{\mathsf{true}})]^{-1} \big)
    \end{align}
    where 
    \begin{align}
        J(\theta) &= -\E_{p(y | \theta)} \left[ \frac{\dif^2}{\dif \theta^2} \log p(y \mid \theta) \right]
    \end{align}
    is the \emph{Fisher information} of parameter $\theta$.
\end{frame}

\begin{frame}{Approximating the marginal likelihood}

The Laplace approximation also offers an approximation of the intractable marginal likelihood,
\begin{align}
    \cL(\mbw_{\mathsf{MAP}}) &= 
    \log p(\mbw_{\mathsf{MAP}}) + \log p(\{y_n\}_{n=1}^N \mid \mbw_{\mathsf{MAP}}, \{\mbx_n\}_{n=1}^N) - \log p(\{y_n\}_{n=1}^N \mid \{\mbx_n\}_{n=1}^N) \\
    & \approx \log \cN(\mbw_{\mathsf{MAP}} \mid \mbw_{\mathsf{MAP}}, \mbSigma) \\
    &= -\frac{P}{2} \log (2 \pi) - \frac{1}{2} \log |\mbSigma|
\end{align}
Rearranging terms,
\begin{align}
    \nonumber
    \log p(\{y_n\}_{n=1}^N \mid \{\mbx_n\}_{n=1}^N) 
    &\approx \log p(\mbw_{\mathsf{MAP}}) + \log p(\{y_n\}_{n=1}^N \mid \mbw_{\mathsf{MAP}}, \{\mbx_n\}_{n=1}^N) \\
    &\qquad + \frac{P}{2} \log (2 \pi) + \frac{1}{2} \log |\mbSigma|
\end{align}

\textbf{Note:} combine this with $\mbSigma \approx \tfrac{1}{N} [J(\mbw_{MAP})]^{-1}$ and $\tfrac{1}{2} \log |\mbSigma| \approx \tfrac{P}{2} \log N + O(1)$ to derive the \emph{Bayesian information criterion (BIC)}, a technique for penalized maximum likelihood estimation.
\end{frame}

\begin{frame}{Exponential family distributions}
    
    Now we'll consider GLMs with \emph{exponential family} observations,
    \begin{align}
        p(y_n \mid \eta_n) &= h(y_n) \exp \left \{\langle t(y_n), \eta_n \rangle - A(\eta_n) \right\},
    \end{align}
    where 
    \begin{itemize}
        \item $h(y_n): \cY \to \reals_+$ is the \emph{base measure},
        \item $t(y_n) \in \reals^T$ are the \textit{sufficient statistics},
        \item $\eta_n \in \reals^T$ are the \textit{natural parameters}, and
        \item $A(\eta_n): \reals^T \to \reals$ is the \textit{log normalizing} function (aka the \emph{partition function}).
    \end{itemize}
    
\end{frame}

\begin{frame}{Example: Bernoulli distribution in exponential family form}
    The Bernoulli distribution is a special case, 
    \begin{align}
        \distBernoulli(y_n \mid p_n) &= p_n^{y_n} \, (1-p_n)^{1 - y_n} \\
        &= \exp \left\{ y_n \log p_n + (1-y_n) \log (1- p_n) \right \} \\
        &= \exp \left\{ y_n \log \frac{p_n}{1 - p_n} + \log (1 - p_n) \right \}  \\
        &= h(y_n) \exp \left\{ y_n \eta_n - A(\eta_n) \right \} 
    \end{align}
    where
    \begin{align}
        h(y_n) &= 1 & 
        t(y_n) &= y_n & 
        \eta_n &= \log \frac{p_n}{1- p_n} &
        A(\eta_n) &= -\log ( 1 - p_n) \\
        & & & & & & &= - \log \left(1 - \frac{e^{\eta_n}}{1 + e^{\eta_n}} \right) \\
        % & & & & & & &= - \log \frac{1}{1 + e^{\eta_n}}  \\
        & & & & & & &= \log \left(1 + e^{\eta_n} \right).
    \end{align}
\end{frame}

\begin{frame}{Gradient of the log normalizer yields the expected sufficient statistics}
    
    By definition,
    \begin{align}
        A(\eta_n) &= \log \int h(y_n) \exp \left \{\langle t(y_n), \eta_n \rangle \right\} \dif y_n
    \end{align}
    so
    \begin{align}
        \nabla_{\eta_n} A(\eta_n) 
        &= \nabla_{\eta_n} \log \int h(y_n) \exp \left \{\langle t(y_n), \eta_n \rangle \right\} \dif y_n \\
        &= \frac{\int h(y_n) \exp \left \{\langle t(y_n), \eta_n \rangle \right\} t(y_n) \dif y_n}{\int h(y_n) \exp \left \{\langle t(y_n), \eta_n \rangle \right\} \dif y_n} \\
        &= \int p(y_n \mid \eta_n) \, t(y_n) \dif y_n \\
        &= \E_{p(y_n | \eta_n)}[t(y_n)]
    \end{align}
    
\end{frame}

\begin{frame}{Hessian of the log normalizer yields the covariance of the sufficient statistics}
    
    Likewise,
    \begin{align}
        \nabla^2_{\eta_n} A(\eta_n) 
        &= \nabla_{\eta_n} \int p(y_n \mid \eta_n) \, t(y_n) \dif y_n \\
        &= \int p(y_n \mid \eta_n) \, t(y_n) \, (t(y_n) - \nabla_{\eta_n} A(\eta_n))^\top \dif y_n \\
        &= \E_{p(y_n | \eta_n)}[t(y_n) t(y_n)^\top ] - \E_{p(y_n | \eta_n)}[t(y_n)] \, \E_{p(y_n | \eta_n)}[t(y_n)]^\top \\
        &= \mathrm{Cov}_{p(y_n | \eta_n)}[t(y_n)]
    \end{align}
    
    Covariances are positive semi-definite, so the \textit{log normalizer is a convex function}.
\end{frame}

\begin{frame}{Exponential family GLMs}
    \begin{itemize}
        \item To construct a generalized linear model with exponential family observations, we set 
    \begin{align}
        \E[y_n \mid \mbx_n] &= f(\mbw^\top \mbx_n).
    \end{align}
    
    \item From above, this implies,
    \begin{align}
        \nabla_{\eta_n} A(\eta_n) &= f(\mbw^\top \mbx_n) \\
        \Rightarrow \eta_n &= [\nabla A]^{-1} \big( f(\mbw^\top \mbx_n) \big),
    \end{align}
    when $\nabla A(\cdot)$ is invertible. (In this case, the exponential family is said to be \emph{minimal}).
    
    \item The \emph{canonical mean function} is $f(\cdot) = \nabla A(\cdot)$ so that $\eta_n = \mbw^\top \mbx_n$.
    
    \item The (canonical) \emph{link function} is the inverse of the (canonical) mean function.
    \end{itemize}
    
\end{frame}

\begin{frame}{Logistic regression revisited}
    Consider the Bernoulli distribution once more. The gradient of the log normalizer is,
    \begin{align}
        \nabla_\eta A(\eta) &= \nabla_\eta \log (1 + e^\eta) 
        = \frac{e^\eta}{1+ e^\eta}
    \end{align}
    This is the logistic function!
    
    Thus, logistic regression with is a Bernoulli GLM with the canonical mean function.
\end{frame}

\begin{frame}{Why care about canonical mean functions?}

(\textbf{Spoiler:} It's 2021 and we have automatic differentiation so it's not such a big deal anymore.)

Canonical mean functions lead to nice math. Consider the log joint probability,
\begin{align}
    \cL(\mbw) &= \log p(\mbw) + \sum_{n=1}^N \langle t(y_n), \eta_n \rangle - A(\eta_n)  + c \\
    &= -\frac{1}{2\sigma^2} \mbw^\top \mbw + \sum_{n=1}^N \langle t(y_n), \mbw^\top \mbx_n \rangle - A(\mbw^\top \mbx_n) + c,
\end{align}
where we have assumed $p(\mbw) = \cN(\mbw \mid \mbzero, \sigma^2 \mbI)$ and canonical mean function so $\eta_n = \mbw^\top \mbx_n$.

The gradient is,
\begin{align}
    \nabla_{\mbw} \cL(\mbw) 
    &= -\frac{1}{\sigma^2}\mbw + \sum_{n=1}^N \langle t(y_n), \mbx_n \rangle - \langle \nabla A(\mbw^\top \mbx_n), \, \mbx_n \rangle\\
    &= -\frac{1}{\sigma^2}\mbw + \sum_{n=1}^N \langle t(y_n) - \E_{p(y_n | \mbw^\top \mbx_n)}[t(y_n)], \, \mbx_n \rangle
\end{align}

\end{frame}

\begin{frame}{Why care about canonical mean functions? II}

In many cases, $t(y_n) = y_n \in \reals$ so
\begin{align}
    \nabla_{\mbw} \cL(\mbw) 
    &= -\frac{1}{\sigma^2}\mbw + \sum_{n=1}^N (y_n - \hat{y}_n) \mbx_n.
\end{align}

And in that case the Hessian is
\begin{align}
    \nabla^2_{\mbw} \cL(\mbw) 
    &= -\frac{1}{\sigma^2}\mbI - \sum_{n=1}^N \nabla^2 A(\mbw^\top \mbx_n) \, \mbx_n \mbx_n^\top \\
    &= -\left( \frac{1}{\sigma^2}\mbI + \sum_{n=1}^N \mathrm{Var}_{p(y_n | \mbw^\top \mbx_n)}[y_n] \, \mbx_n \mbx_n^\top \right)
\end{align}

\end{frame}

\begin{frame}{Why care about canonical mean functions? III}

Now recall the Newton's method updates, written here in terms of the change in weights,
\begin{align}
    \Delta \mbw &= - [\nabla_{\mbw}^2 \cL(\mbw)]^{-1} \nabla_{\mbw} \cL(\mbw) \\
    &= \left[\frac{1}{\sigma^2}\mbI + \sum_{n=1}^N \mathrm{Var}_{p(y_n | \mbw^\top \mbx_n)}[y_n] \, \mbx_n \mbx_n^\top \right]^{-1} \left[ -\frac{1}{\sigma^2}\mbw + \sum_{n=1}^N (y_n - \hat{y}_n) \mbx_n \right]
\end{align}

Letting $\hat{v}_n = \mathrm{Var}_{p(y_n | \mbw^\top \mbx_n)}[y_n]$ and taking the uninformative limit of $\sigma^2 \to \infty$, 
\begin{align}
    \Delta \mbw &=
    \left[\sum_{n=1}^N \hat{v}_n \, \mbx_n \mbx_n^\top \right]^{-1} \left[ \sum_{n=1}^N (y_n - \hat{y}_n) \mbx_n \right] \\
    % &= (\mbX^\top \hat{\mbV} \mbX)^{-1} [\mbX^\top \hat{\mbV} \hat{\mbV}^{-1} (\mby - \hat{\mby})] \\
    &= (\mbX^\top \hat{\mbV} \mbX)^{-1} [\mbX^\top \hat{\mbV} \hat{\mbz}]
\end{align}
where $\hat{\mbV} = \diag([\hat{v}_1, \ldots, \hat{v}_N])$ and $\hat{\mbz} = \hat{\mbV}^{-1} (\mby - \hat{\mby})$. 

This is \emph{iteratively reweighted least squares} (IRLS) with weights $\hat{v}_n$ and targets $\hat{z}_n = \frac{y_n - \hat{y}_n}{\hat{v}_n}$, both of which are functions of the current weights $\mbw$.
\end{frame}

\begin{frame}{Model evaluation via predictive likelihood}
\begin{center}
\includegraphics[width=.85\linewidth]{figures/lap1/boxsloop.jpeg}\\
\end{center} 
\begin{flushright}
{\footnotesize Blei, \textit{Ann. Rev. Stat. App.} 2014.}
\end{flushright}
\end{frame}

\begin{frame}{Measures of predictive accuracy}
    One way to evaluate a model is through the accuracy of its predictions.
    
    For example, given a \textit{point estimate} $\hat{\theta}$, the \textit{mean squared error} summarizes predictive accuracy as,
    \begin{align}
        \frac{1}{N_{\mathsf{test}}} \sum_{n=1}^{N_{\mathsf{test}}} (y_n - \E[y_n \mid \hat{\theta}])^2
    \end{align}
    
    In \textit{probabilistic prediction} (aka \textit{probabilistic forecasting}), we report a distribution over parameters $\theta$. In Bayesian data analysis, this is the posterior distribution $p(\theta \mid \{y_n\}_{n=1}^N)$. 
    
    The \emph{posterior predictive distribution} averages over the posterior to make a prediction,
    \begin{align}
        p(y_{n'} \mid \{y_n\}_{n=1}^N) &= \int p(y_{n'} \mid \theta) \, p(\theta \mid \{y_n\}_{n=1}^N) \dif \theta,
    \end{align}
    weighting parameters by their posterior probability.

\end{frame}

\begin{frame}{Scoring rules}

    A \emph{scoring rule} measures the accuracy of a probabilistic prediction. 
    
    The \emph{logarithmic scoring rule} is unique in that it is both \textit{proper} and \textit{local}. 
    
    The \textit{log predictive likelihood} of new datapoint $y_{n'}$ is,
    \begin{align}
        \log p(y_{n'} \mid \{y_n\}_{n=1}^N) &= \log \int p(y_{n'} \mid \theta) \, p(\theta \mid \{y_n\}_{n=1}^N) \dif \theta.
    \end{align}
    
\end{frame}

\begin{frame}{Averaging over the distribution of future data}
    Ideally, we would measure the expected out-of-sample predictive performance,
    \begin{align}
        \E_{f(y)}[\log p(y \mid \{y_n\}_{n=1}^N)] &= \int \log p(y \mid \{y_n\}_{n=1}^N) \, f(y) \dif y
    \end{align}
    where $f(y)$ is the true, but unknown data-generating distribution.
    
    BDA3 calls this the \emph{expected out-of-sample log predictive density} (elpd). 
    
    But we don't know $f$! Instead, we approximate it with,
    \begin{align}
        \frac{1}{N_{\mathsf{test}}} \sum_{n'=1}^{N_{\mathsf{test}}} \log p(y_{n'} \mid \{y_n\}_{n=1}^N)
    \end{align}
    for some \emph{test dataset} $\{y_{n'}\}_{n'=1}^{N_{\mathsf{test}}}$.
\end{frame}

\begin{frame}{Estimating out-of-sample predictive accuracy with available data}
    
    \begin{itemize}
        \item Naively, the \emph{within-sample} predictive accuracy takes the test set to be the training set. As such, it over-estimates the the predictive accuracy.
    
    \item \emph{Information criteria} like the AIC, DIC, and WAIC try to correct for this bias by penalizing the within-sample estimate.
    
    \item \emph{Cross-validation} splits the data into train and test to approximate the predictive likelihood. 
    
    \item Hardcore Bayesians don't like to withhold data when computing the posterior. Nevertheless, cross-validated predictive log likelihood is the current standard. 
    \end{itemize}

\end{frame}

\begin{frame}{Akaike Information Criterion (AIC)}
    
    The \emph{Akaike information criterion} (AIC) is based on an asymptotic normal approximation to the posterior.
    
    It corrects for the bias of the within-sample log likelihood by accounting for the number of parameters (an approximation to the amnount of overfitting),
    \begin{align}
        -\frac{1}{2} \mathrm{AIC} = \sum_{n=1}^N \log p(y_n \mid \theta_{\mathsf{MLE}}) - K,
    \end{align}
    where $K$ is the number of parameters. 
    
    The AIC makes some sense for linear models with uniform priors, but not so much for more complicated models. 
    
\end{frame}

\begin{frame}{Deviance Information Criterion (DIC)}
    
    The \emph{deviance information criterion} (DIC) makes two changes,
    
    \begin{enumerate}
        \item Replace the MLE with the posterior mean, $\bar{\theta} = \E_{p(\theta | y)}[\theta]$.
        \item Replace $K$ with a bias correction based on the data.
    \end{enumerate}
    Then,
    \begin{align}
        -\frac{1}{2} \mathrm{DIC} = \sum_{n=1}^N \log p(y_n \mid \bar{\theta}) - K_{\mathsf{DIC}}
    \end{align}
    where
    \begin{align}
        K_{\mathsf{DIC}} &= 2 \left( \log \sum_{n=1}^N p(y_n \mid \bar{\theta}) - \E_{p(\theta | y)} \left[ \log \sum_{n=1}^N p(y_n \mid \theta) \right] \right)
    \end{align}
    
    An alternative definition uses,
    \begin{align}
        K_{\mathsf{DIC}} &= 2 \mathrm{Var}_{p(\theta | y)} \left[ \log \sum_{n=1}^N p(y_n \mid \bar{\theta} \right] 
    \end{align}
\end{frame}

\begin{frame}{Watanabe-Akaike Information Criterion (WAIC)}
    
    The \emph{Watanabe-Akaike information criterion} (WAIC) is similar to the DIC with,
    \begin{align}
        K_{\mathsf{WAIC}} &= 2 \left( \log \sum_{n=1}^N \E_{p(\theta | y)} [p(y_n \mid \theta)] - \E_{p(\theta | y)} \left[ \log \sum_{n=1}^N p(y_n \mid \theta) \right] \right)
    \end{align}
    
    An alternative definition uses,
    \begin{align}
        K_{\mathsf{WAIC}} &= \sum_{n=1}^N \mathrm{Var}_{p(\theta | y)} \left[ \log p(y_n \mid \bar{\theta} \right],
    \end{align}
    
    Then, the WAIC is,
    \begin{align}
        -\frac{1}{2} \mathrm{WAIC} &= \sum_{n=1}^N \log \int p(y_n \mid \theta) \, p(\theta \mid \{y_n\}_{n=1}^N \dif \theta - K_{\mathsf{WAIC}}
    \end{align}
    
\end{frame}

\begin{frame}{Leave-one-out cross validation}
    
    \textbf{Idea: } approximate out-of-sample predictive accuracy by randomly splitting the training data. 
    
    \textit{Leave-one-out} cross validation (LOOCV) withholds one datapoint out at a time, computes the posterior given the other $N-1$, and then evaluates predictive likelihood on the held out datapoint.
    
    \begin{align}
        \E_{f(y)}[\log p(y \mid \{y_n\}_{n=1}^N)] &\approx
        \frac{1}{N} \sum_{n=1}^N \log p(y_n \mid \{y_m\}_{m\neq n}).
    \end{align}
    
    For small $N$ (or when using a small number of folds), a bias-correction can be used. (See pg. 175-176 of the book.)
    
    Cross-validated predictive log likelihood estimates are similar to the \emph{jackknife} resampling method in the sense that it is estimating an expectation wrt the unknown data-generating distribution $f$ by resampling the given data. 
    
    Thus, it measures the frequentist properties of a Bayesian procedure.
    
\end{frame}

% \begin{frame}{Next time: Lap 3: Robust/hierarchical models and MCMC}
    
% \end{frame}


\end{document}
