\documentclass[aspectratio=169]{beamer}
\usetheme{simple}

\input{preamble/preamble.tex}
\input{preamble/preamble_math.tex}
% \input{preamble/preamble_acronyms.tex}

\title{STATS271/371: Applied Bayesian Statistics}
\subtitle{Hidden Markov Models (HMMs) and Message Passing Algorithms (Part II)}
\author{Scott Linderman}
\date{\today}


\begin{document}


\maketitle

\begin{frame}{Box's Loop}
\begin{center}
\includegraphics[width=.85\linewidth]{figures/lap1/boxsloop.jpeg}\\
\end{center} 
\begin{flushright}
{\footnotesize Blei, \textit{Ann. Rev. Stat. App.} 2014.}
\end{flushright}
\end{frame}

% \begin{frame}{Lap 7: Hidden Markov Models and Message Passing}
% \begin{itemize}
%     % \item \hyperref[sec:hmms]{\textbf{Model:} Hidden Markov Models}
%     \item \hyperref[sec:mp]{\textbf{Algorithm:} Message Passing}
% \end{itemize}
% \end{frame}

% \section{Model: HMMs}
% \label{sec:hmms}

\begin{frame}{Hidden Markov Models}

Hidden Markov Models (HMMs) assume a particular factorization of the joint distribution on latent states ($z_t$) and observations $(\mbx_t)$. The graphical model looks like this:

\begin{center}
    \includegraphics[width=0.7\textwidth]{figures/lap7/hmm.png}    
\end{center}

This graphical model says that the joint distribution factors as,
\begin{align}
    p(z_{1:T}, \mbx_{1:T}) &= p(z_1) \prod_{t=2}^T p(z_t \mid z_{t-1}) \prod_{t=1}^T p(\mbx_t \mid z_t).
\end{align}

We call this an HMM because $p(z_1) \prod_{t=2}^T p(z_t \mid z_{t-1})$ is a Markov chain.
    
\end{frame}

\begin{frame}{Hidden Markov Models II}
We are interested in questions like:
\begin{itemize}
    \item What is the \textit{posterior marginal} distribution $p(z_t \mid \mbx_{1:T})$?
    \item What is the \textit{posterior pairwise marginal} distribution $p(z_t, z_{t+1} \mid \mbx_{1:T})$?
    \item What is the \textit{posterior mode} $z_{1:T}^\star = \argmax p(z_{1:T} \mid \mbx_{1:T})$?
    \item What is the \textit{predictive distribution} of $p(z_{T+1} \mid \mbx_{1:T})$?
\end{itemize}

\end{frame}


\begin{frame}{State space models}
Note that nothing above assumes that $z_t$ is a discrete random variable!

HMM's are a special case of more general \textbf{state space models} with discrete states. 

State space models assume the same graphical model but allow for arbitrary types of latent states. 

For example, suppose that $\mbz_t \in \mathbb{R}^P$ are continuous valued latent states and that,
\begin{align}
    p(\mbz_{1:T}) &= p(\mbz_1) \prod_{t=2}^T p(\mbz_t \mid \mbz_{t-1}) \\
    &= \cN(\mbz_1 \mid \mbb_1, \mbQ_1) \prod_{t=2}^T \cN(\mbz_t \mid \mbA \mbz_{t-1} + \mbb, \mbQ) 
\end{align}
This is called a \textbf{linear dynamical system} with Gaussian noise. 

\end{frame}

\begin{frame}{Message passing in HMMs}

In the HMM with discrete states, we showed how to compute posterior marginal distributions using message passing,
\begin{align}
    p(z_t \mid \mbx_{1:T}) &\propto \sum_{z_1} \cdots \sum_{z_{t-1}} \sum_{z_{t+1}} \cdots \sum_{z_T} p(z_{1:T}, \mbx_{1:T}) \\
    &= \alpha_t(z_t) \, p(\mbx_t \mid z_t) \, \beta_t(z_t) 
\end{align}
where the \textit{forward and backward messages} are defined recursively
\begin{align}
    \alpha_t(z_t) &= \sum_{z_{t-1}} p(z_t \mid z_{t-1}) \, p(\mbx_{t-1} \mid z_{t-1}) \, \alpha_{t-1}(z_{t-1}) \\
    \beta_t(z_t) &= \sum_{z_{t+1}} \, p(z_{t+1} \mid z_t) \, p(\mbx_{t+1} \mid z_{t+1}) \, \beta_{t+1}(z_{t+1})
\end{align}
The initial conditions are $\alpha_1(z_1) = p(z_1)$ and $\beta_{T}(z_T) = 1$.
    
\end{frame}

\begin{frame}{What do the forward messages tell us?}
\label{slide:fwd_hmm}

The forward messages are equivalent to,
\begin{align}
    \alpha_t(z_t) &= \sum_{z_1} \cdots \sum_{z_{t-1}} p(z_{1:t}, \mbx_{1:t-1}) \\
    &p(z_t, \mbx_{1:t-1}).
\end{align}
The normalized message is the \textit{predictive distribution},
\begin{align}
    \frac{\alpha_t(z_t)}{\sum_{z_t'} \alpha_t(z_t')} &= 
    \frac{p(z_t, \mbx_{1:t-1})}{\sum_{z_t'} p(z_t', \mbx_{1:t-1})} = \frac{p(z_t, \mbx_{1:t-1})}{p(\mbx_{1:t-1})} = p(z_t \mid \mbx_{1:t-1}),
\end{align}
The final normalizing constant yields the marginal likelihood, $\sum_{z_T} \alpha_T(z_T) = p(\mbx_{1:T})$.

% Here, $z_t$ is a discrete random variable so we can think of the message as a vector,
% \begin{align}
%     \mbalpha_t &= [\alpha_{t1}, \ldots, \alpha_{tK}]^\top,
% \end{align}
% with  $\alpha_t(z_t)$ pulling out the $z_t$-th entry in this vector.

\end{frame}

\begin{frame}{Message passing in state space models}

The same recursive algorithm applies (in theory) to any state space model with the same factorization, but the sums are replaced with integrals,
\begin{align}
    p(z_t \mid \mbx_{1:T}) &\propto \int \dif z_1 \cdots \int \dif {z_{t-1}} \int \dif{z_{t+1}} \cdots \int \dif {z_T} \,  p(z_{1:T}, \mbx_{1:T}) \\
    &= \alpha_t(z_t) \, p(\mbx_t \mid z_t) \, \beta_t(z_t) 
\end{align}
where the \textit{forward and backward messages} are defined recursively
\begin{align}
    \alpha_t(z_t) &= \int p(z_t \mid z_{t-1}) \, p(\mbx_{t-1} \mid z_{t-1}) \, \alpha_{t-1}(z_{t-1}) \dif {z_{t-1}} \\
    \beta_t(z_t) &= \int p(z_{t+1} \mid z_t) \, p(\mbx_{t+1} \mid z_{t+1}) \, \beta_{t+1}(z_{t+1}) \dif {z_{t+1}} 
\end{align}
The initial conditions are $\alpha_1(z_1) = p(z_1)$ and $\beta_{T}(z_T) = 1$.
    
\end{frame}

\begin{frame}[t]{Message passing in a linear dynamical system}
\textbf{Exercise:} Consider an LDS with Gaussian noise and assume that $p(\mbx_t \mid \mbz_t) = \cN(\mbx_t \mid \mbC \mbz_t + \mbd, \mbR)$. Derive the forward message $\alpha_t(\mbz_t)$ under the inductive hypothesis that $\alpha_{t-1}(\mbz_{t-1}) \propto \cN(\mbz_{t-1} \mid \mbmu_{t-1}, \mbSigma_{t-1})$.
\end{frame}

\begin{frame}[t]{Message passing in nonlinear dynamical systems}
\textbf{Question:} What if $p(\mbz_t \mid \mbz_{t-1}) = \cN(z_t \mid f(\mbz_{t-1}), \mbQ)$ for some nonlinear function $f$? 

\end{frame}

\begin{frame}{Sequential Monte Carlo}
Recall that the forward messages are proportional to the predictive distributions $p(\mbz_t \mid \mbx_{1:t-1})$. We can view the forward recursions as an expectation,
\begin{align}
    \alpha_t(\mbz_t) &= \int p(\mbz_t \mid z_{t-1}) \, p(\mbx_{t-1} \mid \mbz_{t-1}) \, \alpha_{t-1}(\mbz_{t-1}) \dif {\mbz_{t-1}} \\
    &\propto \E_{\mbz_{t-1} \sim p(\mbz_{t-1} \mid \mbx_{1:t-2})} \left[ p(\mbz_t \mid z_{t-1}) \, p(\mbx_{t-1} \mid \mbz_{t-1}) \right] 
\end{align}
One natural idea is to approximate this expectation with Monte Carlo,
\begin{align}
    \hat{\alpha}_t(\mbz_t) 
    &\approx \frac{1}{S} \sum_{s=1}^S \left[ w_{t-1}^{(s)} \, p(\mbz_t \mid \mbz_{t-1}^{(s)})  \right] 
\end{align}
where we have defined the \textbf{weights} $w_{t-1}^{(s)} \triangleq p(\mbx_{t-1} \mid \mbz_{t-1}^{(s)})$.

How do we sample $\mbz_{t-1}^{(s)} \iid{\sim} p(\mbz_{t-1} \mid \mbx_{1:t-2})$? Let's sample the normalized $\hat{\alpha}_{t-1}(\mbz_{t-1})$ instead!
\end{frame}

\begin{frame}{Sequential Monte Carlo II}
The normalizing constant is,
\begin{align}
   \int \hat{\alpha}_{t-1}(\mbz_{t-1}) \dif \mbz_{t-1} &= 
   \frac{1}{S} \sum_{s=1}^S w_{t-2}^{(s)} \int p(\mbz_{t-1} \mid \mbz_{t-2}^{(s)}) \dif \mbz_{t-1} 
   = \frac{1}{S} \sum_{s=1}^S w_{t-2}^{(s)}.
\end{align}
Use this to define the \textit{normalized forward message} (i.e. the Monte Carlo estimate of the predictive distribution) is,
\begin{align}
    \bar{\alpha}_{t-1}(\mbz_{t-1}) &\triangleq \frac{\hat{\alpha}_{t-1}(\mbz_{t-1})}{\int \hat{\alpha}_{t-1}(\mbz_{t-1}') \dif \mbz_{t-1}'}
    = \sum_{s=1}^S \bar{w}_{t-2}^{(s)} \, p(\mbz_{t-1} \mid \mbz_{t-2}^{(s)})
\end{align}
where $\bar{w}_{t-2}^{(s)} = \frac{w_{t-2}^{(s)}}{\sum_{s'} w_{t-2}^{(s')}}$ is the normalized weight of sample $\mbz_{t-2}^{(s)}$.

\textbf{The normalized forward message is just a mixture distribution with weights $\bar{w}_{t-2}^{(s)}$!}
\end{frame}

\begin{frame}{Putting it all together}
Combining the above, we have the following algorithm for the forward pass:
\begin{enumerate}
    \item Let $\bar{\alpha}_1(\mbz_1) = p(z_1)$
    \item For $t=1, \ldots, T$:
    \begin{enumerate}[a.]
        \item Sample $\mbz_{t}^{(s)} \iid{\sim} \bar{\alpha}_t(\mbz_t)$ for $s=1, \ldots, S$
        \item Compute weights $w_t^{(s)} = p(\mbx_t \mid \mbz_t^{(s)})$ and normalize $\bar{w}_t^{(s)} = w_t^{(s)} / \sum_{s'} w_t^{(s')}$.
        \item Compute normalized forward message $\bar{\alpha}_{t+1}(\mbz_{t+1}) = \sum_{s=1}^S \bar{w}_t^{(s)} p(\mbz_{t+1} \mid \mbz_t^{(s)})$.
    \end{enumerate}
\end{enumerate}

This is called \textbf{sequential Monte Carlo} (SMC) using the model dynamics as the proposal.

Note that Step 2a can \textbf{resample} the same $\mbz_{t-1}^{(s)}$ multiple times according to its weight. 

\textbf{Question:} How can you approximate the marginal likelihood $p(\mbx_{1:T})$ using the weights? \textit{Hint: look back to Slide~\ref{slide:fwd_hmm}}.

\end{frame}

\begin{frame}{Generalizations}
    \begin{itemize}
        \item Instead of sampling $\bar{\alpha}_{t}(\mbz_{t})$, we could have sampled with a \textbf{proposal distribution} $r(\mbz_{t} \mid \mbz_{t-1}^{(s)})$ instead and corrected for it by defining the weights to be,
        \begin{align}
            w_t^{(s)} &= \frac{p(\mbz_{t} \mid \mbz_{t-1}^{(s)}) \, p(\mbx_t \mid \mbz_t)}{r(\mbz_{t} \mid \mbz_{t-1}^{(s)})}
        \end{align}
        Moreover, the proposal distribution can ``look ahead'' to future data $\mbx_t$.
    \end{itemize}
\end{frame}

\begin{frame}[t,allowframebreaks]
        \frametitle{References}
        \bibliographystyle{unsrtnat}
        \bibliography{refs.bib}
\end{frame}

\end{document}